{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "tags": []
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --user google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure you restart the session after installing the packages to restart the run-time kernel."
      ],
      "metadata": {
        "id": "0s8uQ23rusWY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXHfaVS66_01"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lslYAvw37JGQ",
        "tags": [],
        "executionInfo": {
          "status": "ok",
          "timestamp": 1708637095432,
          "user_tz": 480,
          "elapsed": 56216,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1nfXrqRxVX"
      },
      "source": [
        "### Load the Gemini Pro model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2998506fe6d1",
        "tags": [],
        "executionInfo": {
          "status": "ok",
          "timestamp": 1708637099775,
          "user_tz": 480,
          "elapsed": 75,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "model = GenerativeModel(\"gemini-1.5-pro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIl7R_jBUsaC"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Send a text prompt to the model. The Gemini Pro (`gemini-pro`) model provides a streaming response mechanism. With this approach, you don't need to wait for the complete response; you can start processing fragments as soon as they're accessible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAo-UsfZECGF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(\"What are the colors of the rainbow?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us8idXnVyQ97"
      },
      "source": [
        "#### Try a simple prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmAZQW1GyQ97",
        "tags": []
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Create a numbered list of 10 items.\n",
        "Each item in the list should be a trend in the tech industry.\n",
        "Each trend should be less than 5 words.\"\"\"\n",
        "\n",
        "responses = model.generate_content(prompt, stream=True)\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDK4XLdz3Oqv"
      },
      "source": [
        "#### Model parameters\n",
        "\n",
        "Every prompt you send to the model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_2ann-F3WTo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def query(prompt):\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=0.9,\n",
        "        top_p=1,\n",
        "        top_k=32,\n",
        "        candidate_count=1,\n",
        "        max_output_tokens=8192,\n",
        "    )\n",
        "\n",
        "    responses = model.generate_content(\n",
        "        prompt,\n",
        "        generation_config=generation_config,\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    for response in responses:\n",
        "        print(response.text, end=\"\")\n",
        "\n",
        "query(\"What are the best hikes in Oregon?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA66gm6ttw-a"
      },
      "source": [
        "#### Interactive interface\n",
        "\n",
        "We can use IPython's simple UI elements to implement an interactive interface. Experiment with queries against the model using it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import widgets, Output\n",
        "\n",
        "output = Output()\n",
        "# Define button click event\n",
        "def on_button_click(b):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        prompt = prompt_input.value\n",
        "        print(\"[+] Query: \" + prompt)\n",
        "        print(f\"Summary: {query(prompt)}\")\n",
        "\n",
        "# Create widgets\n",
        "prompt_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter query',\n",
        "    description='Query:',\n",
        "    disabled=False\n",
        ")\n",
        "# Create a button\n",
        "button = widgets.Button(description=\"Run\")\n",
        "button.on_click(on_button_click)\n",
        "# Display widgets\n",
        "display(prompt_input, button)\n",
        "display(output)"
      ],
      "metadata": {
        "id": "L3g2CfqQnKmw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m113",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
